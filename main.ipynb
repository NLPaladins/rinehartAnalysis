{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NLP - Project 1\n",
    "## Rinehart Analysis\n",
    "**Team**: *Jean Merlet, Konstantinos Georgiou, Matt Lane*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the **[README](https://github.com/NLPaladins/rinehartAnalysis/blob/main/README.md)**\n",
    "\n",
    "\n",
    "Or the current **[TODO](https://github.com/NLPaladins/rinehartAnalysis/blob/main/TODO.md)** list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Jupyter Widgets\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from IPython.display import display\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clone project?\n",
      "(If you do this you will ovewrite local changes on other files e.g. configs)\n",
      "Not needed if you're not on Google Collab\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e5acd9195a4d32b057371fbf9c6f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Yes, clone', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clone the repository if you're in Google Collab\n",
    "def clone_project(is_collab: bool = False):\n",
    "    print(\"Cloning Project..\")\n",
    "    !git clone https://github.com/NLPaladins/rinehartAnalysis.git\n",
    "    print(\"Project cloned.\")\n",
    "       \n",
    "print(\"Clone project?\")\n",
    "print(\"(If you do this you will ovewrite local changes on other files e.g. configs)\")\n",
    "print(\"Not needed if you're not on Google Collab\")\n",
    "btn = widgets.Button(description=\"Yes, clone\")\n",
    "btn.on_click(clone_project)\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you on Google Collab?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9836a4edda8c4a9d98e84f84e3b8c3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Yes', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clone the repository if you're in Google Collab\n",
    "def change_dir(is_collab: bool = False):\n",
    "    try:\n",
    "        print(\"Changing dir..\")\n",
    "        os.chdir('/content/rinehartAnalysis')\n",
    "        print('done')\n",
    "        print(\"Current dir:\")\n",
    "        print(os.getcwd())\n",
    "        print(\"Dir Contents:\")\n",
    "        print(os.listdir())\n",
    "    except Exception:\n",
    "        print(\"Error: Project not cloned\")\n",
    "       \n",
    "print(\"Are you on Google Collab?\")\n",
    "btn = widgets.Button(description=\"Yes\")\n",
    "btn.on_click(change_dir)\n",
    "display(btn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At any point, to save changes\n",
    "click **File > Save a copy on Gihtub**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now go to files on the left, and open:\n",
    "- rinehartAnalysis/confs/proj_1.yml\n",
    "\n",
    "(Ctr+s) to save changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Load Libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import argparse\n",
    "from importlib import reload as reload_lib\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# Custom libs\n",
    "from nlp_libs import Configuration, ColorizedLogger, ProcessedBook\n",
    "# Import this way the libs you want to dynamically change and reload \n",
    "# import nlp_libs.books.processed_book as books_lib # Comment out until Class is finalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Overview\n",
    "All the libraries are located under *\"\\<project root>/nlp_libs\"*\n",
    "- ***ProcessedBook***: Loc: **books/processed_book.py**, Desc: *Book Pre-processor*\n",
    "- ***Configuration***: Loc: **configuration/configuration.py**, Desc: *Configuration Loader*\n",
    "- ***ColorizedLogger***: Loc: **fancy_logger/colorized_logger.py**, Desc: *Logger with formatted text capabilities*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path of configuration and log save path\n",
    "config_path = \"confs/proj_1.yml\"  # Open files > confs > proj_1.yml to edit temporalily. Commit to save permanently\n",
    "# !cat \"$config_path\"\n",
    "log_path = \"logs/proj_1.log\"  # Open files > logs > proj_1.log to debug logs of previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 12:47:22 FancyLogger  INFO     \u001b[1m\u001b[37mLogger is set. Log file path: /Users/96v/Documents/DSE/nlp/rinehartAnalysis/logs/proj_1.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The logger\n",
    "logger = ColorizedLogger(logger_name='Notebook', color='cyan')\n",
    "ColorizedLogger.setup_logger(log_path=log_path, debug=False, clear_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 12:47:24 Config       INFO     \u001b[1m\u001b[37mConfiguration file loaded successfully from path: /Users/96v/Documents/DSE/nlp/rinehartAnalysis/confs/proj_1.yml\u001b[0m\n",
      "2021-09-26 12:47:24 Config       INFO     \u001b[1m\u001b[37mConfiguration Tag: proj1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': {'crime_type': 'example',\n",
      "       'detective': ['man1', 'man2'],\n",
      "       'suspects': ['man3', 'man4'],\n",
      "       'url': 'https://gutenberg.org/files/9931/9931-0.txt'},\n",
      " 'The_After_House': {'crime_type': 'example',\n",
      "                     'detective': ['man1', 'man2'],\n",
      "                     'suspects': ['man3', 'man4'],\n",
      "                     'url': 'https://gutenberg.org/files/2358/2358-0.txt'},\n",
      " 'The_Case_of_Jennie_Brice': {'crime_type': 'example',\n",
      "                              'detective': ['man1', 'man2'],\n",
      "                              'suspects': ['man3', 'man4'],\n",
      "                              'url': 'https://gutenberg.org/cache/epub/11127/pg11127.txt'},\n",
      " 'The_Circular_Staircase': {'crime_type': 'example',\n",
      "                            'detectives': ['Rachel Innes'],\n",
      "                            'suspects': ['Liddy',\n",
      "                                         'Halsey',\n",
      "                                         'Gertrude',\n",
      "                                         'Paul Armstrong',\n",
      "                                         'Doctor Walker',\n",
      "                                         'Louise Armstrong',\n",
      "                                         'Arnold Armstrong',\n",
      "                                         'Mrs. Ralston',\n",
      "                                         'Thomas Johnson',\n",
      "                                         'Aunt Ray',\n",
      "                                         'Mary Anne',\n",
      "                                         'Burke',\n",
      "                                         'Joe Jefferson',\n",
      "                                         'Anne Watson',\n",
      "                                         'Eliza Klinefelter',\n",
      "                                         'Beulah',\n",
      "                                         'Jack Bailey',\n",
      "                                         'Mr. Jarvis',\n",
      "                                         'Warner',\n",
      "                                         'Mr. Jamieson',\n",
      "                                         'Mr. Harton',\n",
      "                                         'Rosie',\n",
      "                                         'Sam Bohannon',\n",
      "                                         'Beatrice Fairfax',\n",
      "                                         'Mrs. Ogden Fitzhugh',\n",
      "                                         'Mr. Trautman',\n",
      "                                         'Doctor Stewart',\n",
      "                                         'Doctor Wainwright',\n",
      "                                         'Alexander Graham',\n",
      "                                         'Nina Carrington',\n",
      "                                         'Doctor Willoughby',\n",
      "                                         'Riggs',\n",
      "                                         'Covingtons',\n",
      "                                         'Sam Huston',\n",
      "                                         'Anne Endicott',\n",
      "                                         'Barbara Fitzhugh',\n",
      "                                         'Lucien Wallace',\n",
      "                                         'Anna Whitcomb',\n",
      "                                         'Madame Sweeny',\n",
      "                                         'Sam Bohannon',\n",
      "                                         'Ella Stewart',\n",
      "                                         'Annie Morton',\n",
      "                                         'Matthew Geist',\n",
      "                                         'Mattie Bliss',\n",
      "                                         'Doctor Boyle'],\n",
      "                            'url': 'https://www.gutenberg.org/files/434/434-0.txt'},\n",
      " 'The_Man_in_Lower_Ten': {'crime_type': 'example',\n",
      "                          'detectives': ['man1', 'man2'],\n",
      "                          'suspects': ['man3', 'man4'],\n",
      "                          'url': 'https://www.gutenberg.org/files/1869/1869-0.txt'}}\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration\n",
    "conf = Configuration(config_src=config_path)\n",
    "# Get the books dict\n",
    "books = conf.get_config('data_loader')['config']['books']\n",
    "pprint(books)  # Pretty print the books dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from typing import *\n",
    "\n",
    "\n",
    "class ProcessedBook:\n",
    "    num_map = [(1000, 'M'), (900, 'CM'), (500, 'D'), (400, 'CD'),\n",
    "               (100, 'C'), (90, 'XC'), (50, 'L'), (40, 'XL'),\n",
    "               (10, 'X'), (9, 'IX'), (5, 'V'), (4, 'IV'), (1, 'I')]\n",
    "    title: str\n",
    "    url: str\n",
    "    detectives: List[str]\n",
    "    suspects: List[str]\n",
    "    crime_type: str\n",
    "\n",
    "    def __init__(self, title: str, metadata: Dict, make_lower: bool = True):\n",
    "        \"\"\"\n",
    "        raw holds the books as a single string.\n",
    "        clean holds the books as a list of lowercase lines starting\n",
    "        from the first chapter and ending with the last sentence.\n",
    "        \"\"\"\n",
    "        self.title = title\n",
    "        self.url = metadata['url']\n",
    "        self.detectives = metadata['detectives']\n",
    "        self.suspects = metadata['suspects']\n",
    "        self.crime_type = metadata['crime_type']\n",
    "        self.raw = self.read_book_from_proj_gut(self.url)\n",
    "        if make_lower:\n",
    "            lines = self.raw.lower()\n",
    "        else:\n",
    "            lines = self.raw\n",
    "            \n",
    "        lines = re.sub(r'\\r\\n', r'\\n', lines)\n",
    "        lines = re.findall(r'.*(?=\\n)',  lines)        \n",
    "        \n",
    "        self.lines = self.clean_lines(lines=lines)\n",
    "        self.clean = self.get_clean_book(make_lower=make_lower)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_book_from_proj_gut(book_url: str) -> str:\n",
    "        req = urllib.request.Request(book_url)\n",
    "        client = urllib.request.urlopen(req)\n",
    "        page = client.read()\n",
    "        return page.decode('utf-8')\n",
    "\n",
    "    def get_clean_book(self, make_lower: bool = True) -> List[str]:\n",
    "        if make_lower:\n",
    "            lines = self.raw.lower()\n",
    "        else:\n",
    "            lines = self.raw\n",
    "\n",
    "        lines = re.sub(r'\\r\\n', r'\\n', lines)\n",
    "        lines = re.findall(r'.*(?=\\n)',  lines)        \n",
    "\n",
    "        lines = self.clean_lines(lines)\n",
    "        chapters = self.lines_to_chapters(lines)\n",
    "        return chapters\n",
    "\n",
    "    def clean_lines(self, lines: List[str]) -> List[str]:\n",
    "        clean_lines = []\n",
    "        start = False\n",
    "        for line in lines:\n",
    "            if re.match(r'^chapter i\\.', line, re.IGNORECASE):\n",
    "                clean_lines.append(line)\n",
    "                start = True\n",
    "                continue\n",
    "            if not start:\n",
    "                continue\n",
    "            if re.match(r'^\\*\\*\\* end of the project gutenberg ebook', line, re.IGNORECASE):\n",
    "                break\n",
    "            if self.pass_clean_filter(line):\n",
    "                clean_lines.append(line)\n",
    "        return clean_lines\n",
    "\n",
    "    @staticmethod\n",
    "    def lines_to_chapters(lines: List[str]) -> List[str]:\n",
    "        chapters = []\n",
    "        sentences = []\n",
    "        current_sent = ''\n",
    "        for i, line in enumerate(lines):\n",
    "            # add chapter as 1st sentence\n",
    "            if re.match(r'^chapter [ivxlcdm]+\\.$', line, re.IGNORECASE):\n",
    "                if sentences:\n",
    "                    chapters.append(sentences)\n",
    "                sentences = [line]\n",
    "                add_chapter_title = True\n",
    "                continue\n",
    "            # add chapter title as 2nd sentence\n",
    "            elif add_chapter_title:\n",
    "                sentences.append(line)\n",
    "                add_chapter_title = False\n",
    "                continue\n",
    "            sents = re.findall(r' *((?:mr\\.|mrs.|[^\\.\\?!])*)(?<!mr)(?<!mrs)[\\.\\?!]', line, re.IGNORECASE)\n",
    "            # if no sentence end is detected\n",
    "            if not sents:\n",
    "                if current_sent == '':\n",
    "                    current_sent = line\n",
    "                else:\n",
    "                    current_sent += ' ' + line\n",
    "            # if at least one sentence end is detected\n",
    "            else:\n",
    "                for group in sents:\n",
    "                    if current_sent != '':\n",
    "                        current_sent += ' ' + group\n",
    "                        sentences.append(current_sent)\n",
    "                    else:\n",
    "                        sentences.append(group)\n",
    "                    current_sent = ''\n",
    "                # set the next sentence to its start if there is one\n",
    "                sent_end = re.search(r'(?<!mr)(?<!mrs)[\\.\\?!] ((?:mr\\.|mrs\\.|[^\\.\\?!])*)$', line, re.IGNORECASE)\n",
    "                if sent_end is not None:\n",
    "                    current_sent = sent_end.groups()[0]\n",
    "        return chapters\n",
    "\n",
    "    @staticmethod\n",
    "    def pass_clean_filter(line: str) -> bool:\n",
    "        # removing the illustration lines and empty lines\n",
    "        # can add other filters here as needed\n",
    "        if line == '' or re.match(r'illustration:|\\[illustration\\]', line, re.IGNORECASE):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def get_characters_per_chapter(self, chapter): \n",
    "        found_character_list = []\n",
    "        singular_or_multiple_names = '[A-Z][a-z][A-Z]?[a-z][A-Z]?[a-z]+(?:(?:\\s|,|.|\\.\\s)[A-Z][a-z][A-Z]?[a-z][A-Z]?[a-z]+)?(?:\\s[A-Z][a-z][A-Z]?[a-z][A-Z]?[a-z]+)?(?:\\s[A-Z][a-z][A-Z]?[a-z][A-Z]?[a-z]+)?'\n",
    "        # TODO: Executive decision occurred on the search string to ignore the \n",
    "        #       first word of each sentence as regex cannot differentiate between \n",
    "        #       a singular word and a name.  However, this does introduce a curious\n",
    "        #       thing with McKnight in the man in the lower ten, such that we get him\n",
    "        #.      as \"Knight\". \n",
    "        search_string = re.compile(fr'(?<!“)(?<!‘)(?<!^)({singular_or_multiple_names})')\n",
    "        #get characters per sentence in chapter\n",
    "        for sentence in chapter:\n",
    "            res = re.findall(search_string, sentence)\n",
    "            found_character_list.append(res)\n",
    "\n",
    "        unique_characters = list(np.concatenate(found_character_list))\n",
    "        return found_character_list, unique_characters\n",
    "      \n",
    "        \n",
    "    ##\n",
    "    ## @Warning: Currently only works with all text as upper case.  \n",
    "    ##\n",
    "    def get_all_characters_per_novel(self):\n",
    "        preceding_words_to_ditch = [\n",
    "            'After', 'Although', 'And', 'As','At',\n",
    "            'Before', 'Both', 'But', 'Did', 'For', \n",
    "            'Good', 'Had', 'Has', 'Home', 'If', 'Is',\n",
    "            'Leaving', 'Like', 'No', 'Nice', 'Old', 'On', 'Or',\n",
    "            'Poor', 'Send', 'So', 'That', 'Tell', 'The', 'Thank', \n",
    "            'To', 'Was', 'Whatever', 'When', 'Where', 'While', \n",
    "            'With','Your', 'View', \n",
    "            #Specific Places\n",
    "            'African', 'Brewing', 'Hospital', 'Zion', 'New','Country', \n",
    "            'Greenwood', 'Western', 'American', 'Bar', 'Chestnut', 'Queen', \n",
    "            'Summitville', 'Union', 'City', \"Japan\",\"Europe\",\"Company\",\n",
    "            \"Street\",\"Station\",\"Bank\",\"Weekly\", \"ville\",\"Providence\",\n",
    "            \"Creek\",\"Brewing\", 'California', 'Italian', 'London', 'French',\n",
    "            'Scotland'\n",
    "        ]\n",
    "        \n",
    "\n",
    "        book_by_chapter = self.lines_to_chapters(self.lines)\n",
    "        \n",
    "        totalUniqueList = []\n",
    "        characterProgressionList = []\n",
    "        for chapter in book_by_chapter: \n",
    "\n",
    "            characterProgression, uniqueCharacters = self.get_characters_per_chapter(chapter)\n",
    "\n",
    "            characterProgressionList.append(characterProgression)\n",
    "            totalUniqueList = [*totalUniqueList, *uniqueCharacters]\n",
    "\n",
    "        totalUnique = set(totalUniqueList)\n",
    "        \n",
    "        joined_preceding_words_to_lose = '|'.join(preceding_words_to_ditch)\n",
    "        #not even preceding - just ditch them if they're within the \"name\"\n",
    "        preceding_word_to_lose_regex = fr'^(?!.*({joined_preceding_words_to_lose})).*'\n",
    "        regex = re.compile(preceding_word_to_lose_regex)\n",
    "\n",
    "        filtered_people = list(filter(regex.match, totalUnique))\n",
    "        \n",
    "        return filtered_people, characterProgressionList\n",
    "        \n",
    "    def get_chapter(self, chapter: int) -> str:\n",
    "        return self.clean[chapter - 1]\n",
    "\n",
    "    def extract_character_names(self):\n",
    "        lines_by_chapter = self.lines_to_chapters(self.lines)\n",
    "        for chapter in lines_by_chapter: \n",
    "            print(chapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Circlular Staircase\n",
    "title, metadata = list(books.items())[0]\n",
    "staircase = ProcessedBook(title=title, metadata=metadata, make_lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mCHAPTER XXXIII.\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mAT THE FOOT OF THE STAIRS\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mAs I drove rapidly up to the house from Casanova Station in the hack, I saw the detective Burns loitering across the street from the Walker place\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mSo Jamieson was putting the screws on—lightly now, but ready to give them a twist or two, I felt certain, very soon\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mThe house was quiet\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mTwo steps of the circular staircase had been pried off, without result, and beyond a second message from Gertrude, that Halsey insisted on coming home and they would arrive that night, there was nothing new\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mMr. Jamieson, having failed to locate the secret room, had gone to the village\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mI learned afterwards that he called at Doctor Walker’s, under pretense of an attack of acute indigestion, and before he left, had inquired about the evening trains to the city\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mHe said he had wasted a lot of time on the case, and a good bit of the mystery was in my imagination\u001b[0m\n",
      "2021-09-26 12:47:30 Notebook     INFO     \u001b[1m\u001b[36mThe doctor was under the impression that the house was guarded day and night\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(staircase.get_chapter(33)):\n",
    "  if i == 10: break\n",
    "  logger.info(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Lower Ten\n",
    "title, metadata = list(books.items())[0]\n",
    "lower_ten = ProcessedBook(title=title, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mThe raw length of this book as a string is 410135\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mThis book has 33 chapters\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mchapter i. - i take a country house\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mThere are 117 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 21.307692307692307 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mchapter ii. - a link cuff-button\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mThere are 142 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 16.260563380281692 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mchapter iii. - mr. john bailey appears\u001b[0m\n",
      "2021-09-26 12:47:32 Notebook     INFO     \u001b[1m\u001b[36mThere are 104 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 14.663461538461538 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mchapter iv. - where is halsey?\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mThere are 143 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 12.23076923076923 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mchapter v. - gertrude’s engagement\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mThere are 140 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 13.228571428571428 words\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'The raw length of this book as a string is {len(lower_ten.raw)}')\n",
    "logger.info(f'This book has {len(lower_ten.clean)} chapters\\n')\n",
    "for i, chapter in enumerate(lower_ten.clean):\n",
    "  if i == 5: break\n",
    "  logger.info(f'{chapter[0]} - {chapter[1]}')\n",
    "  logger.info(f'There are {len(chapter)} sentences in this chapter.')\n",
    "  num_words = []\n",
    "  for sent in chapter:\n",
    "    num_words.append(len(sent.split(' ')))\n",
    "  avg_words = np.mean(num_words)\n",
    "  logger.info(f'The average sentence length in this chapter is {avg_words} words\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mchapter xv.\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mliddy gives the alarm\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mthe next day, friday, gertrude broke the news of her stepfather’s death to louise\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mshe did it as gently as she could, telling her first that he was very ill, and finally that he was dead\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mlouise received the news in the most unexpected manner, and when gertrude came out to tell me how she had stood it, i think she was almost shocked\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36m“she just lay and stared at me, aunt ray,” she said\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36m“do you know, i believe she is glad, glad\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mand she is too honest to pretend anything else\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36mwhat sort of man was mr. paul armstrong, anyhow\u001b[0m\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[1m\u001b[36m“he was a bully as well as a rascal, gertrude,” i said\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate(lower_ten.get_chapter(15)):\n",
    "  if i == 10: \n",
    "    break\n",
    "  logger.info(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-09-26 12:47:33 Notebook     INFO     \u001b[4m\u001b[33mBook: The_Circular_Staircase\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThe raw length of this book as a string is 410135\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThis book has 33 chapters\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mchapter i. - i take a country house\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThere are 117 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 21.307692307692307 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mchapter ii. - a link cuff-button\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThere are 142 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 16.260563380281692 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mchapter iii. - mr. john bailey appears\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThere are 104 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 14.663461538461538 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mchapter iv. - where is halsey?\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThere are 143 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 12.23076923076923 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mchapter v. - gertrude’s engagement\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThere are 140 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 13.228571428571428 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mchapter xv.\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mliddy gives the alarm\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mthe next day, friday, gertrude broke the news of her stepfather’s death to louise\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mshe did it as gently as she could, telling her first that he was very ill, and finally that he was dead\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mlouise received the news in the most unexpected manner, and when gertrude came out to tell me how she had stood it, i think she was almost shocked\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36m“she just lay and stared at me, aunt ray,” she said\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36m“do you know, i believe she is glad, glad\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mand she is too honest to pretend anything else\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36mwhat sort of man was mr. paul armstrong, anyhow\u001b[0m\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[1m\u001b[36m“he was a bully as well as a rascal, gertrude,” i said\u001b[0m\n",
      "\n",
      "2021-09-26 12:47:35 Notebook     INFO     \u001b[4m\u001b[33mBook: The_Man_in_Lower_Ten\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThe raw length of this book as a string is 381299\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThis book has 30 chapters\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mchapter i. - i go to pittsburg\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThere are 198 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 13.944444444444445 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mchapter ii. - a torn telegram\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThere are 157 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 17.076433121019107 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mchapter iii. - across the aisle\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThere are 141 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 10.617021276595745 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mchapter iv. - numbers seven and nine\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThere are 138 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 13.347826086956522 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mchapter v. - the woman in the next car\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThere are 98 sentences in this chapter.\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mThe average sentence length in this chapter is 12.285714285714286 words\n",
      "\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mchapter xv.\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mliddy gives the alarm\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mthe next day, friday, gertrude broke the news of her stepfather’s death to louise\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mshe did it as gently as she could, telling her first that he was very ill, and finally that he was dead\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mlouise received the news in the most unexpected manner, and when gertrude came out to tell me how she had stood it, i think she was almost shocked\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36m“she just lay and stared at me, aunt ray,” she said\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36m“do you know, i believe she is glad, glad\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mand she is too honest to pretend anything else\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36mwhat sort of man was mr. paul armstrong, anyhow\u001b[0m\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[1m\u001b[36m“he was a bully as well as a rascal, gertrude,” i said\u001b[0m\n",
      "\n",
      "2021-09-26 12:47:38 Notebook     INFO     \u001b[4m\u001b[33mBook: The_After_House\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'detectives'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3g/0q_1cdr55qsgk59qbrs83m78dw2x85/T/ipykernel_40081/1718194132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Book: {title}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yellow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'underline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mcurrent_book\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessedBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m# Raw length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The raw length of this book as a string is {len(current_book.raw)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/3g/0q_1cdr55qsgk59qbrs83m78dw2x85/T/ipykernel_40081/3319924984.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, title, metadata, make_lower)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detectives'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuspects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'suspects'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrime_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'crime_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'detectives'"
     ]
    }
   ],
   "source": [
    "# Example of running it on all the books\n",
    "processed_books = {}\n",
    "for title, metadata in books.items():\n",
    "  logger.nl()\n",
    "  logger.info(f\"Book: {title}\", color='yellow', attrs=['underline'])\n",
    "  current_book = ProcessedBook(title=title, metadata=metadata)\n",
    "  # Raw length\n",
    "  logger.info(f'The raw length of this book as a string is {len(current_book.raw)}')\n",
    "  # Number of chapters\n",
    "  logger.info(f'This book has {len(current_book.clean)} chapters\\n')\n",
    "  # Sententences per chapter\n",
    "  for i, chapter in enumerate(current_book.clean):\n",
    "    if i == 5: \n",
    "      break\n",
    "    logger.info(f'{chapter[0]} - {chapter[1]}')\n",
    "    logger.info(f'There are {len(chapter)} sentences in this chapter.')\n",
    "    num_words = []\n",
    "    for sent in chapter:\n",
    "      num_words.append(len(sent.split(' ')))\n",
    "    avg_words = np.mean(num_words)\n",
    "    logger.info(f'The average sentence length in this chapter is {avg_words} words\\n')\n",
    "  # Chapter 15\n",
    "  for i, sent in enumerate(lower_ten.get_chapter(15)):\n",
    "    if i == 10: break\n",
    "    logger.info(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Circlular Staircase\n",
    "# title, metadata = list(books.items())[0]\n",
    "# staircaseUpper = ProcessedBook(title=title, metadata=metadata, make_lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputStr = [ 'Miss Gertrude',\n",
    "#  'Miss Gertrude Innes',\n",
    "#  'Miss Innes',\n",
    "#  'Miss Liddy',\n",
    "#  'Miss Louise',\n",
    "#  'Arnold Armstrong',\n",
    "#  'Miss Rachel',\n",
    "#  'Mr. Armstrong',\n",
    "#  'Mr. Arnold',\n",
    "#  'Mr. Arnold Armstrong',\n",
    "#  'Louise Armstrong',\n",
    "#  'Fanny Armstrong',\n",
    "#  'Peter Armstrong',\n",
    "#  'Miss Armstrong',\n",
    "#  'Mrs. Armstrong',\n",
    "#  'Paul Armstrong',\n",
    "#     'Anne Endicott',\n",
    "#   'Anne Haswell',\n",
    "#   'Anne Watson',\n",
    "#   'Mary Anne'\n",
    "# ]\n",
    "\n",
    "# output = {\n",
    "#     'Miss Gertrude Innes': [\n",
    "#         'Miss Gertrude',\n",
    "#         'Miss Gertrude Innes',\n",
    "#         'Miss Innes'\n",
    "#     ],\n",
    "#     'Miss Liddy': [ 'Miss Liddy'  ], \n",
    "#     'Miss Louise':[ 'Miss Louise' ],\n",
    "#     'Miss Rachel':[ 'Miss Rachel' ],\n",
    "#     'Mr. Arnold Armstrong': [\n",
    "#         'Mr. Arnold Armstrong',\n",
    "#         'Arnold Armstrong',\n",
    "#         'Mr. Armstrong',\n",
    "#         'Mr. Arnold',\n",
    "#     ], \n",
    "#     'Anne Endicott': ['Anne Endicott'],\n",
    "#     'Anne Haswell': ['Anne Haswell'],\n",
    "#     'Anne Watson': ['Anne Watson'],\n",
    "#     'Mary Anne': ['Mary Anne']\n",
    "\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = re.findall(r'^(?:Mr\\.|Mrs\\.|Miss|Doctor)', 'Mr. Arnold Armstrong')\n",
    "# print(title)\n",
    "# title = re.findall(r'^(?:Mr\\.|Mrs\\.|Miss|Doctor)', 'Doctor Arnold Armstrong')\n",
    "# print(title)\n",
    "# no_title = re.findall(r'^(?:Mr\\.|Mrs\\.|Miss|Doctor)', 'Arnold Armstrong')\n",
    "# print(no_title)\n",
    "# name_split_no_title = re.findall(r'(?!Mr\\.|Mrs\\.|Miss|Doctor)[A-Z][a-z]+', 'Mr. Arnold Armstrong')\n",
    "# name_split_no_title if len(name_split_no_title) == 1 else [name_split_no_title[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surname = re.findall(r'[A-Z][a-z]+$', 'Mr. Arnold Armstrong')\n",
    "# print(surname)\n",
    "# surname = re.findall(r'[A-Z][a-z]+$', 'Arnold Armstrong')\n",
    "# print(surname)\n",
    "# surname = re.findall(r'[A-Z][a-z]+$', 'Miss Armstrong')\n",
    "# print(surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexString = r'[A-Z][a-z]+(?=\\s)'\n",
    "# surname = re.findall(regexString, 'Mr. Arnold Armstrong')\n",
    "# print(surname)\n",
    "# surname = re.findall(regexString, 'Arnold Armstrong')\n",
    "# print(surname)\n",
    "# surname = re.findall(regexString, 'Miss Armstrong')\n",
    "# print(surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createNamedDictionary(personList): \n",
    "#     personList.sort(key=len, reverse=True)\n",
    "#     name_dictionary = {}\n",
    "#     hasKey=False\n",
    "\n",
    "#     for potential_name_key in personList:         \n",
    "#         title = re.findall(r'^(?:Mr\\.|Mrs\\.|Miss|Doctor)', potential_name_key)\n",
    "#         name_split_no_title = re.findall(r'(?!Mr\\.|Mrs\\.|Miss|Doctor)[A-Z][a-z]+', potential_name_key)\n",
    "#         original_length_no_title = len(name_split_no_title)\n",
    "        \n",
    "#         if len(name_split_no_title) > 1:\n",
    "#             surname = name_split_no_title[1]\n",
    "#             name_split_no_title = [name_split_no_title[0]]\n",
    "        \n",
    "#         print(len(name_split_no_title))\n",
    "#         print(name_split_no_title)\n",
    "#         for name in personList:\n",
    "#             if name in name_dictionary.keys() or (len(name_dictionary.values()) > 0 and\n",
    "#                                                   name in np.concatenate(list(name_dictionary.values()))):\n",
    "# #                 print(\"Continuing on \", name)\n",
    "#                 continue\n",
    "\n",
    "#             if re.match(fr\".*({'|'.join(name_split_no_title)}).*\", name):\n",
    "# #                 print('\\t ',re.match(fr\".*({'|'.join(name_split_no_title)}).*\", name))\n",
    "\n",
    "#                 if original_length_no_title == 1: \n",
    "#                     continue\n",
    "                    \n",
    "\n",
    "# #                 if len(name) > len(potential_name_key): \n",
    "# #                     raise(\"This shouldn't happen with the way the sorting works\")\n",
    "#                 else:\n",
    "#                     if potential_name_key not in name_dictionary.keys() and name not in name_dictionary.keys(): \n",
    "# #                         print('\\t Key:', potential_name_key, \"\\t\\t Value: \", potential_name_key,  )\n",
    "#                         name_dictionary[potential_name_key] = [ potential_name_key ]\n",
    "#                     elif potential_name_key in name_dictionary.keys(): \n",
    "# #                             print('\\t Key:', potential_name_key, \"\\t\\t Value: \", name )\n",
    "# #                             print('\\tt name_split_no_title: ', name_split_no_title )\n",
    "# #                             if re.match(fr'.*(?!{surname}).*$', name) and len(name_split_no_title) == len(re.findall(r'(?!Mr\\.|Mrs\\.|Miss|Doctor)[A-Z][a-z]+', name)):\n",
    "# #                                 print('\\t\\t >>>>>>>>>>>. SURNAME DOES NOT MATCH!!!')\n",
    "# #                                 print('\\t\\t >>>>>>>>>>>.potential_name_key, name')\n",
    "\n",
    "                                \n",
    "#                             name_dictionary[potential_name_key] = [\n",
    "#                                 *name_dictionary[potential_name_key], \n",
    "#                                 name\n",
    "#                             ]                    \n",
    "                    \n",
    "# #                     print('key: ', potential_name_key, '\\tvalue', potential_name_key)\n",
    "# #                     name_dictionary[potential_name_key] = [ potential_name_key ]\n",
    "#                 continue\n",
    "        \n",
    "#     return name_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_unambiguous_name_list(unique_person_list, surname_list): \n",
    "    unambiguous_name_list = []\n",
    "    print(surname_list)\n",
    "    for name in unique_person_list:\n",
    "        name_split_no_title = re.findall(r'(?!Mr\\.|Mrs\\.|Miss|Doctor|Aunt)[A-Z][a-z]+', name)\n",
    "        if len(name_split_no_title) == 0: \n",
    "            continue\n",
    "#         print(name, name_split_no_title)\n",
    "        first_name = name_split_no_title[0]\n",
    "\n",
    "\n",
    "        if first_name not in surname_list:\n",
    "            unambiguous_name_list.append(name)\n",
    "        else: \n",
    "            print(f\"Name {name} is ambiguous. Not processing\")\n",
    "    return unambiguous_name_list\n",
    "\n",
    "def create_named_dictionary(unique_person_list): \n",
    "    title_regex = r'^(?:Mr\\.|Mrs\\.|Miss|Doctor)'\n",
    "    no_title_regex = r'(?!Mr\\.|Mrs\\.|Miss|Doctor)[A-Z][a-z]+'\n",
    "    unique_person_list.sort(key=len, reverse=True)\n",
    "    unique_person_key = {}\n",
    "    name_dictionary = {}\n",
    "    \n",
    "    surname_list = extract_surnames(unique_person_list)\n",
    "    print(surname_list)\n",
    "    unambiguous_person_list = get_unambiguous_name_list(unique_person_list, surname_list)\n",
    "#     print(unambiguous_person_list)\n",
    "    alias_dictionary = createNamedDictionary(unambiguous_person_list)\n",
    "    \n",
    "    return alias_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surnames = extract_surnames(unique_person_list)\n",
    "# names_to_remove_from_surnames = ['Anne', 'Rachel', 'Rosie', 'Thomas']\n",
    "# surnames = list(filter(lambda x: x not in names_to_remove_from_surnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# unique_person_list.sort(key=len, reverse=True)\n",
    "# unique_person_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS THE ALIASING CODE: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surnames(unique_person_list): \n",
    "    surname_list = []\n",
    "    # first pass: go through, get break of first / lasts\n",
    "    for name in unique_person_list:\n",
    "        name_split_no_title = re.findall(r'(?!Mr\\.|Mrs\\.|Miss|Doctor)[A-Z][a-z]+', name)\n",
    "        surname = '' if len(name_split_no_title) <= 1 else name_split_no_title[1]\n",
    "\n",
    "        if surname != '' and surname: \n",
    "            surname_list.append(surname)\n",
    "\n",
    "    return set(surname_list)\n",
    "\n",
    "\n",
    "def obtain_aliases_for_book(unique_person_list): \n",
    "    unique_person_list.sort(key=len, reverse=True)\n",
    "    alias_dictionary = {}\n",
    "    title_regex = r'^(?:Mr\\.|Mrs\\.|Miss|Doctor)'\n",
    "    name_with_no_title_regex = r'(?!Mr\\.|Mrs\\.|Miss|Doctor|Aunt)[A-Z][a-z]+'\n",
    "    surnames = extract_surnames(unique_person_list)\n",
    "    \n",
    "    for personIdx in range(len(unique_person_list)): \n",
    "        if len(alias_dictionary.keys()) == 0: \n",
    "            alias_dictionary[unique_person_list[personIdx]] = [unique_person_list[personIdx]]\n",
    "\n",
    "\n",
    "        comparator_person = unique_person_list[personIdx]\n",
    "        title_comparator_person = re.findall(title_regex, comparator_person)\n",
    "        name_split_no_title_comparator_person = re.findall(name_with_no_title_regex, comparator_person)\n",
    "\n",
    "        if len(alias_dictionary.values()) > 0 and comparator_person in list(np.concatenate(list(alias_dictionary.values()))):\n",
    "            continue\n",
    "#         print(comparator_person)\n",
    "#         print(title_comparator_person, name_split_no_title_comparator_person)\n",
    "\n",
    "\n",
    "        if len(name_split_no_title_comparator_person) == 0: \n",
    "    #         raise('ZERO? ', comparator_person, name_split_no_title_comparator_person)\n",
    "            continue\n",
    "        surname_comparator_person = '' if len(name_split_no_title_comparator_person) <= 1 else name_split_no_title_comparator_person[1]\n",
    "        first_name_comparator_person = name_split_no_title_comparator_person[0]\n",
    "\n",
    "        for next_person_index in range(personIdx, len(unique_person_list)):     \n",
    "            next_person = unique_person_list[next_person_index]\n",
    "            title_next_person = re.findall(title_regex, next_person)\n",
    "            name_split_no_title_next_person = re.findall(name_with_no_title_regex, next_person)\n",
    "\n",
    "            if len(name_split_no_title_next_person) == 0: \n",
    "#                 print(f'NEXT PERSON ZERO:{next_person} ')\n",
    "                continue\n",
    "\n",
    "            if comparator_person == next_person: \n",
    "                alias_dictionary[comparator_person] = [ comparator_person ]\n",
    "#                 print(\"COnTINUING BECAUSE ADDED STUFF\")\n",
    "                continue\n",
    "\n",
    "            surname_next_person = '' if len(name_split_no_title_next_person) <= 1 else name_split_no_title_next_person[1]\n",
    "            first_name_next_person = name_split_no_title_next_person[0]\n",
    "\n",
    "#             print('\\t\\t',next_person)\n",
    "#             print('\\t\\t',title_next_person, name_split_no_title_next_person, f'SURNAME {len(name_split_no_title_next_person)} {surname_next_person}')\n",
    "\n",
    "            if first_name_next_person in surnames: \n",
    "#                 print(\"\\t::::::::NAME IN SURNAMES::::::::\", title_next_person, name_split_no_title_next_person)\n",
    "                continue\n",
    "\n",
    "            if first_name_comparator_person == first_name_next_person and len(first_name_next_person) > 0: \n",
    "\n",
    "                ### SURNAME CHECK GOES HERE *** NEED TO MAKE ABSOLUTELY SURE:\n",
    "                if surname_comparator_person != '' and surname_next_person == '': \n",
    "                    alias_dictionary[comparator_person] = [*alias_dictionary[comparator_person] , next_person ]\n",
    "                    continue\n",
    "\n",
    "\n",
    "                if surname_comparator_person != '' and surname_next_person != '' and surname_comparator_person != surname_next_person: \n",
    "                    continue\n",
    "\n",
    "\n",
    "#                 print(\"\\t\\t**********WHOOOOOP**********************\",len(first_name_next_person), first_name_next_person, first_name_comparator_person)\n",
    "                alias_dictionary[comparator_person] = [*alias_dictionary[comparator_person] , next_person ]\n",
    "                continue\n",
    "    return alias_dictionary\n",
    "\n",
    "def get_dictionary_of_named_occurrences(character_progression): \n",
    "    namecount = {}\n",
    "    for chapter in character_progression: \n",
    "        for line in chapter: \n",
    "            for element in line: \n",
    "                if element not in namecount.keys():\n",
    "                    namecount[element] = 1\n",
    "                else: \n",
    "                    namecount[element] = namecount[element]+1\n",
    "                    \n",
    "    sorted_dict = {}\n",
    "    for key_value in sorted(namecount.items(), key=lambda x: x[1], reverse=True): \n",
    "        sorted_dict[key_value[0]] = key_value[1]\n",
    "    return sorted_dict\n",
    "\n",
    "def get_counts_per_book_by_index(book_index): \n",
    "    title, metadata = list(books.items())[book_index]\n",
    "    print(title)\n",
    "    book = ProcessedBook(title=title, metadata=metadata, make_lower=False)\n",
    "    unique_characters, character_progression = book.get_all_characters_per_novel()\n",
    "    character_reference_counts = get_dictionary_of_named_occurrences(character_progression)\n",
    "    return character_reference_counts\n",
    "\n",
    "def create_alias_occurrence_dictionary(aliases, named_occurrences): \n",
    "    new_named_occurrences = {}\n",
    "    for key in aliases.keys(): \n",
    "        for named_occurrence in named_occurrences.keys(): \n",
    "            if named_occurrence in aliases[key]: \n",
    "                key_exists = key in new_named_occurrences.keys()\n",
    "                occurrences = named_occurrences[named_occurrence]\n",
    "                new_named_occurrences[key] =  occurrences if not key_exists else  new_named_occurrences[key] + occurrences \n",
    "\n",
    "    return new_named_occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lower Ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Circlular Staircase\n",
    "title, metadata = list(books.items())[1]\n",
    "lower_ten = ProcessedBook(title=title, metadata=metadata, make_lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, progression = lower_ten.get_all_characters_per_novel()\n",
    "lower_ten_aliases = obtain_aliases_for_book(unique)\n",
    "named_occurrences = get_dictionary_of_named_occurrences(progression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Henry Pinckney Sullivan': ['Henry Pinckney Sullivan'],\n",
       " 'Harry Pinckney Sullivan': ['Harry Pinckney Sullivan', 'Harry'],\n",
       " 'Westinghouse Electric': ['Westinghouse Electric'],\n",
       " 'Wilson Budd Hotchkiss': ['Wilson Budd Hotchkiss'],\n",
       " 'Alleghany Mountains': ['Alleghany Mountains'],\n",
       " 'Government English': ['Government English', 'Government'],\n",
       " 'Camberwell Beauty': ['Camberwell Beauty'],\n",
       " 'Lawrence Blakeley': ['Lawrence Blakeley', 'Lawrence'],\n",
       " 'Monsieur Blakeley': ['Monsieur Blakeley'],\n",
       " 'Purloined Letter': ['Purloined Letter'],\n",
       " 'Simon Harrington': ['Simon Harrington'],\n",
       " 'Justice Springer': ['Justice Springer'],\n",
       " 'Doctor Williams': ['Doctor Williams'],\n",
       " 'Richey McKnight': ['Richey McKnight', 'Richey'],\n",
       " 'Fish Commission': ['Fish Commission'],\n",
       " 'Francis Johnson': ['Francis Johnson'],\n",
       " 'Great Unwashed': ['Great Unwashed'],\n",
       " 'Henry Sullivan': ['Henry Sullivan'],\n",
       " 'Harry Sullivan': ['Harry Sullivan', 'Harry'],\n",
       " 'Great Unkissed': ['Great Unkissed'],\n",
       " 'Dorothy Browne': ['Dorothy Browne', 'Dorothy'],\n",
       " 'Blanche Conway': ['Blanche Conway'],\n",
       " 'Janet MacLure': ['Janet MacLure', 'Janet'],\n",
       " 'John Flanders': ['John Flanders'],\n",
       " 'Cinematograph': ['Cinematograph'],\n",
       " 'Pennsylvania': ['Pennsylvania'],\n",
       " 'Bernard Shaw': ['Bernard Shaw'],\n",
       " 'Alice Curtis': ['Alice Curtis', 'Alice'],\n",
       " 'Pittsburgers': ['Pittsburgers'],\n",
       " 'Timeo Danaos': ['Timeo Danaos'],\n",
       " 'Miss Gardner': ['Miss Gardner'],\n",
       " 'Grand Rapids': ['Grand Rapids'],\n",
       " 'Commonwealth': ['Commonwealth'],\n",
       " 'John Gilmore': ['John Gilmore'],\n",
       " 'Gulf Stream': ['Gulf Stream'],\n",
       " 'Alleghanies': ['Alleghanies'],\n",
       " 'Christmases': ['Christmases'],\n",
       " 'Chevy Chase': ['Chevy Chase'],\n",
       " 'Monongahela': ['Monongahela'],\n",
       " 'Monte Carlo': ['Monte Carlo'],\n",
       " 'Conan Doyle': ['Conan Doyle'],\n",
       " 'Seal Harbor': ['Seal Harbor'],\n",
       " 'Alison West': ['Alison West', 'Alison'],\n",
       " 'Edgar Allan': ['Edgar Allan'],\n",
       " 'Cannonball': ['Cannonball'],\n",
       " 'Florentine': ['Florentine'],\n",
       " 'Harrington': ['Harrington'],\n",
       " 'Englishman': ['Englishman'],\n",
       " 'Harrisburg': ['Harrisburg'],\n",
       " 'September': ['September'],\n",
       " 'Hotchkiss': ['Hotchkiss'],\n",
       " 'Conductor': ['Conductor'],\n",
       " 'Pittsburg': ['Pittsburg'],\n",
       " 'Incubator': ['Incubator'],\n",
       " 'Baltimore': ['Baltimore'],\n",
       " 'Miss West': ['Miss West'],\n",
       " 'Christmas': ['Christmas'],\n",
       " 'Christian': ['Christian'],\n",
       " 'Mechanics': ['Mechanics'],\n",
       " 'Annapolis': ['Annapolis'],\n",
       " 'Sherlock': ['Sherlock'],\n",
       " 'Irishman': ['Irishman'],\n",
       " 'Sullivan': ['Sullivan'],\n",
       " 'Seiberts': ['Seiberts'],\n",
       " 'MacLures': ['MacLures'],\n",
       " 'Rafferty': ['Rafferty'],\n",
       " 'Euphemia': ['Euphemia'],\n",
       " 'Egyptian': ['Egyptian'],\n",
       " 'Blakeley': ['Blakeley'],\n",
       " 'Austrian': ['Austrian'],\n",
       " 'Virginia': ['Virginia'],\n",
       " 'Hercules': ['Hercules'],\n",
       " 'Richmond': ['Richmond'],\n",
       " 'Waterloo': ['Waterloo'],\n",
       " 'Hawaiian': ['Hawaiian'],\n",
       " 'Curtises': ['Curtises'],\n",
       " 'Dallases': ['Dallases'],\n",
       " 'Bohemian': ['Bohemian'],\n",
       " 'Saturday': ['Saturday'],\n",
       " 'McKnight': ['McKnight'],\n",
       " 'Gaboriau': ['Gaboriau'],\n",
       " 'Riviera': ['Riviera'],\n",
       " 'Granger': ['Granger'],\n",
       " 'Neither': ['Neither'],\n",
       " 'Wyoming': ['Wyoming'],\n",
       " 'Laurels': ['Laurels'],\n",
       " 'Robison': ['Robison'],\n",
       " 'Yorkers': ['Yorkers'],\n",
       " 'Gilmore': ['Gilmore'],\n",
       " 'Klopton': ['Klopton'],\n",
       " 'Johnson': ['Johnson'],\n",
       " 'Bronson': ['Bronson'],\n",
       " 'Altoona': ['Altoona'],\n",
       " 'Bokhara': ['Bokhara'],\n",
       " 'Masonic': ['Masonic'],\n",
       " 'Columns': ['Columns'],\n",
       " 'Pullman': ['Pullman'],\n",
       " 'Heavens': ['Heavens'],\n",
       " 'Cresson': ['Cresson'],\n",
       " 'Springs': ['Springs'],\n",
       " 'Florida': ['Florida'],\n",
       " 'Jenkins': ['Jenkins'],\n",
       " 'English': ['English'],\n",
       " 'Russian': ['Russian'],\n",
       " 'Candida': ['Candida'],\n",
       " 'Obadiah': ['Obadiah'],\n",
       " 'Bermuda': ['Bermuda'],\n",
       " 'Science': ['Science'],\n",
       " 'Health': ['Health'],\n",
       " 'County': ['County'],\n",
       " 'Wisdom': ['Wisdom'],\n",
       " 'Sahara': ['Sahara'],\n",
       " 'Bridge': ['Bridge'],\n",
       " 'Russia': ['Russia'],\n",
       " 'Arnold': ['Arnold'],\n",
       " 'Miller': ['Miller'],\n",
       " 'Monday': ['Monday'],\n",
       " 'George': ['George'],\n",
       " 'Knight': ['Knight'],\n",
       " 'Jennie': ['Jennie'],\n",
       " 'Kanaka': ['Kanaka'],\n",
       " 'Wesson': ['Wesson'],\n",
       " 'Browne': ['Browne'],\n",
       " 'Conway': ['Conway'],\n",
       " 'Ladies': ['Ladies'],\n",
       " 'Scotch': ['Scotch'],\n",
       " 'Stogie': ['Stogie'],\n",
       " 'Angora': ['Angora'],\n",
       " 'Carter': ['Carter'],\n",
       " 'Heaton': ['Heaton'],\n",
       " 'Thomas': ['Thomas'],\n",
       " 'Africa': ['Africa'],\n",
       " 'Junior': ['Junior'],\n",
       " 'Stuart': ['Stuart'],\n",
       " 'Lollie': ['Lollie'],\n",
       " 'Curtis': ['Curtis'],\n",
       " 'Honest': ['Honest'],\n",
       " 'Elwell': ['Elwell'],\n",
       " 'Heaven': ['Heaven'],\n",
       " 'Dallas': ['Dallas'],\n",
       " 'Friday': ['Friday'],\n",
       " 'Pirate': ['Pirate'],\n",
       " 'Sunday': ['Sunday'],\n",
       " 'Wreck': ['Wreck'],\n",
       " 'Eliza': ['Eliza'],\n",
       " 'Blobs': ['Blobs'],\n",
       " 'Shack': ['Shack'],\n",
       " 'Death': ['Death'],\n",
       " 'Satan': ['Satan'],\n",
       " 'Italy': ['Italy'],\n",
       " 'Godey': ['Godey'],\n",
       " 'Polly': ['Polly'],\n",
       " 'Dolly': ['Dolly'],\n",
       " 'Blake': ['Blake'],\n",
       " 'Cuban': ['Cuban'],\n",
       " 'Smith': ['Smith'],\n",
       " 'Jonah': ['Jonah'],\n",
       " 'Allie': ['Allie'],\n",
       " 'Patch': ['Patch'],\n",
       " 'Arabs': ['Arabs'],\n",
       " 'Thing': ['Thing'],\n",
       " 'Woman': ['Woman'],\n",
       " 'Fount': ['Fount'],\n",
       " 'Flier': ['Flier'],\n",
       " 'Peter': ['Peter'],\n",
       " 'East': ['East'],\n",
       " 'Ergo': ['Ergo'],\n",
       " 'Lord': ['Lord'],\n",
       " 'Cubs': ['Cubs'],\n",
       " 'York': ['York'],\n",
       " 'Fido': ['Fido'],\n",
       " 'West': ['West'],\n",
       " 'Kirk': ['Kirk'],\n",
       " 'Rich': ['Rich'],\n",
       " 'Turk': ['Turk'],\n",
       " 'Shaw': ['Shaw'],\n",
       " 'Arms': ['Arms'],\n",
       " 'Mall': ['Mall'],\n",
       " 'King': ['King'],\n",
       " 'Seat': ['Seat'],\n",
       " 'Book': ['Book'],\n",
       " 'Rome': ['Rome'],\n",
       " 'Fate': ['Fate'],\n",
       " 'Balu': ['Balu'],\n",
       " 'Peck': ['Peck'],\n",
       " 'Girl': ['Girl'],\n",
       " 'Lady': ['Lady'],\n",
       " 'Jove': ['Jove']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_ten_aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Henry Pinckney Sullivan': 3,\n",
       " 'Harry Pinckney Sullivan': 2,\n",
       " 'Westinghouse Electric': 1,\n",
       " 'Wilson Budd Hotchkiss': 2,\n",
       " 'Alleghany Mountains': 1,\n",
       " 'Government English': 2,\n",
       " 'Camberwell Beauty': 1,\n",
       " 'Lawrence Blakeley': 16,\n",
       " 'Monsieur Blakeley': 1,\n",
       " 'Purloined Letter': 1,\n",
       " 'Simon Harrington': 8,\n",
       " 'Justice Springer': 1,\n",
       " 'Doctor Williams': 2,\n",
       " 'Richey McKnight': 30,\n",
       " 'Fish Commission': 1,\n",
       " 'Francis Johnson': 1,\n",
       " 'Great Unwashed': 1,\n",
       " 'Henry Sullivan': 1,\n",
       " 'Harry Sullivan': 2,\n",
       " 'Great Unkissed': 2,\n",
       " 'Dorothy Browne': 7,\n",
       " 'Blanche Conway': 2,\n",
       " 'Janet MacLure': 2,\n",
       " 'John Flanders': 1,\n",
       " 'Cinematograph': 1,\n",
       " 'Pennsylvania': 1,\n",
       " 'Bernard Shaw': 1,\n",
       " 'Alice Curtis': 3,\n",
       " 'Pittsburgers': 1,\n",
       " 'Timeo Danaos': 1,\n",
       " 'Miss Gardner': 1,\n",
       " 'Grand Rapids': 1,\n",
       " 'Commonwealth': 2,\n",
       " 'John Gilmore': 7,\n",
       " 'Gulf Stream': 1,\n",
       " 'Alleghanies': 1,\n",
       " 'Christmases': 1,\n",
       " 'Chevy Chase': 1,\n",
       " 'Monongahela': 1,\n",
       " 'Monte Carlo': 1,\n",
       " 'Conan Doyle': 1,\n",
       " 'Seal Harbor': 6,\n",
       " 'Alison West': 67,\n",
       " 'Edgar Allan': 1,\n",
       " 'Cannonball': 5,\n",
       " 'Florentine': 1,\n",
       " 'Harrington': 13,\n",
       " 'Englishman': 1,\n",
       " 'Harrisburg': 1,\n",
       " 'September': 5,\n",
       " 'Hotchkiss': 104,\n",
       " 'Conductor': 1,\n",
       " 'Pittsburg': 28,\n",
       " 'Incubator': 6,\n",
       " 'Baltimore': 17,\n",
       " 'Miss West': 19,\n",
       " 'Christmas': 1,\n",
       " 'Christian': 2,\n",
       " 'Mechanics': 1,\n",
       " 'Annapolis': 1,\n",
       " 'Sherlock': 1,\n",
       " 'Irishman': 1,\n",
       " 'Sullivan': 76,\n",
       " 'Seiberts': 2,\n",
       " 'MacLures': 1,\n",
       " 'Rafferty': 1,\n",
       " 'Euphemia': 9,\n",
       " 'Egyptian': 1,\n",
       " 'Blakeley': 44,\n",
       " 'Austrian': 1,\n",
       " 'Virginia': 1,\n",
       " 'Hercules': 1,\n",
       " 'Richmond': 13,\n",
       " 'Waterloo': 1,\n",
       " 'Hawaiian': 1,\n",
       " 'Curtises': 1,\n",
       " 'Dallases': 1,\n",
       " 'Bohemian': 2,\n",
       " 'Saturday': 4,\n",
       " 'McKnight': 135,\n",
       " 'Gaboriau': 1,\n",
       " 'Riviera': 1,\n",
       " 'Granger': 7,\n",
       " 'Neither': 1,\n",
       " 'Wyoming': 1,\n",
       " 'Laurels': 4,\n",
       " 'Robison': 3,\n",
       " 'Yorkers': 1,\n",
       " 'Gilmore': 9,\n",
       " 'Klopton': 53,\n",
       " 'Johnson': 33,\n",
       " 'Bronson': 45,\n",
       " 'Altoona': 1,\n",
       " 'Bokhara': 4,\n",
       " 'Masonic': 1,\n",
       " 'Columns': 1,\n",
       " 'Pullman': 11,\n",
       " 'Heavens': 3,\n",
       " 'Cresson': 15,\n",
       " 'Springs': 1,\n",
       " 'Florida': 1,\n",
       " 'Jenkins': 1,\n",
       " 'English': 2,\n",
       " 'Russian': 1,\n",
       " 'Candida': 3,\n",
       " 'Obadiah': 3,\n",
       " 'Bermuda': 1,\n",
       " 'Science': 3,\n",
       " 'Health': 3,\n",
       " 'County': 1,\n",
       " 'Wisdom': 1,\n",
       " 'Sahara': 1,\n",
       " 'Bridge': 1,\n",
       " 'Russia': 3,\n",
       " 'Arnold': 4,\n",
       " 'Miller': 1,\n",
       " 'Monday': 4,\n",
       " 'George': 2,\n",
       " 'Knight': 54,\n",
       " 'Jennie': 7,\n",
       " 'Kanaka': 1,\n",
       " 'Wesson': 1,\n",
       " 'Browne': 1,\n",
       " 'Conway': 12,\n",
       " 'Ladies': 1,\n",
       " 'Scotch': 1,\n",
       " 'Stogie': 3,\n",
       " 'Angora': 1,\n",
       " 'Carter': 17,\n",
       " 'Heaton': 1,\n",
       " 'Thomas': 1,\n",
       " 'Africa': 1,\n",
       " 'Junior': 1,\n",
       " 'Stuart': 7,\n",
       " 'Lollie': 10,\n",
       " 'Curtis': 21,\n",
       " 'Honest': 1,\n",
       " 'Elwell': 1,\n",
       " 'Heaven': 7,\n",
       " 'Dallas': 10,\n",
       " 'Friday': 2,\n",
       " 'Pirate': 3,\n",
       " 'Sunday': 8,\n",
       " 'Wreck': 1,\n",
       " 'Eliza': 2,\n",
       " 'Blobs': 10,\n",
       " 'Shack': 2,\n",
       " 'Death': 1,\n",
       " 'Satan': 1,\n",
       " 'Italy': 1,\n",
       " 'Godey': 1,\n",
       " 'Polly': 1,\n",
       " 'Dolly': 3,\n",
       " 'Blake': 1,\n",
       " 'Cuban': 1,\n",
       " 'Smith': 1,\n",
       " 'Jonah': 1,\n",
       " 'Allie': 6,\n",
       " 'Patch': 1,\n",
       " 'Arabs': 1,\n",
       " 'Thing': 1,\n",
       " 'Woman': 1,\n",
       " 'Fount': 1,\n",
       " 'Flier': 4,\n",
       " 'Peter': 1,\n",
       " 'East': 2,\n",
       " 'Ergo': 1,\n",
       " 'Lord': 4,\n",
       " 'Cubs': 1,\n",
       " 'York': 3,\n",
       " 'Fido': 1,\n",
       " 'West': 6,\n",
       " 'Kirk': 4,\n",
       " 'Rich': 12,\n",
       " 'Turk': 1,\n",
       " 'Shaw': 2,\n",
       " 'Arms': 1,\n",
       " 'Mall': 1,\n",
       " 'King': 1,\n",
       " 'Seat': 1,\n",
       " 'Book': 1,\n",
       " 'Rome': 2,\n",
       " 'Fate': 1,\n",
       " 'Balu': 1,\n",
       " 'Peck': 7,\n",
       " 'Girl': 1,\n",
       " 'Lady': 2,\n",
       " 'Jove': 3}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_alias_occurrence_dictionary(lower_ten_aliases, named_occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @kostas already did this\n",
    "def find_murder_moment(book): \n",
    "    book_by_chapter = book.lines_to_chapters(book.lines)\n",
    "\n",
    "    murder_words = ['murder', 'death']\n",
    "    murder_string = '|'.join(murder_words)\n",
    "    murder_regex = re.compile(fr\".*({murder_string}).*\")\n",
    "    for chapter in book_by_chapter:\n",
    "        for line in chapter: \n",
    "            found = re.findall(murder_regex, line, re.IGNORECASE)\n",
    "            if len(found) > 1: \n",
    "                print(\"FOUND: \", found)\n",
    "                print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dictionary_of_named_occurrences(character_progression): \n",
    "#     namecount = {}\n",
    "#     for chapter in character_progression: \n",
    "#         for line in chapter: \n",
    "#             for element in line: \n",
    "#                 if element not in namecount.keys():\n",
    "#                     namecount[element] = 1\n",
    "#                 else: \n",
    "#                     namecount[element] = namecount[element]+1\n",
    "                    \n",
    "#     sorted_dict = {}\n",
    "#     for key_value in sorted(namecount.items(), key=lambda x: x[1], reverse=True): \n",
    "#         sorted_dict[key_value[0]] = key_value[1]\n",
    "#     return sorted_dict\n",
    "\n",
    "# def get_counts_per_book_by_index(book_index): \n",
    "#     title, metadata = list(books.items())[book_index]\n",
    "#     print(title)\n",
    "#     book = ProcessedBook(title=title, metadata=metadata, make_lower=False)\n",
    "#     unique_characters, character_progression = book.get_all_characters_per_novel()\n",
    "#     character_reference_counts = get_dictionary_of_named_occurrences(character_progression)\n",
    "#     return character_reference_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circular Staircase: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Circlular Staircase\n",
    "get_counts_per_book_by_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Man in Lower Ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Circlular Staircase\n",
    "get_counts_per_book_by_index(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Breaking Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Circlular Staircase\n",
    "book_index = 2\n",
    "title, metadata = list(books.items())[book_index]\n",
    "print(title)\n",
    "book = ProcessedBook(title=title, metadata=metadata, make_lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = book.raw\n",
    "lines = re.sub(r'\\r\\n', r'\\n', raw)\n",
    "lines = re.findall(r'.*(?=\\n)',  lines)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines\n",
    "book.clean_lines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Know how Women Are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_counts_per_book_by_index(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Window at the White Cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_counts_per_book_by_index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
